{"cells":[{"source":"","metadata":{},"id":"914000c0-a5fe-4289-a359-bc1965f0c39e","cell_type":"code","execution_count":null,"outputs":[]},{"source":"#  DataCamp Blog Scraper\n\n## üéØ –ü—Ä–∏–∑–Ω–∞—á–µ–Ω–Ω—è\nPython —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ–≥–æ –∑–±–æ—Ä—É —Å—Ç–∞—Ç–µ–π –∑ –±–ª–æ–≥—É DataCamp (datacamp.com/blog).\n\n## üîß –û—Å–Ω–æ–≤–Ω—ñ —Ñ—É–Ω–∫—Ü—ñ—ó\n\n### **1. `scrape_datacamp_alternative()` ‚Äî RSS –ø–∞—Ä—Å–∏–Ω–≥**\n- **–ú–µ—Ç–æ–¥:** –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è RSS feed (https://www.datacamp.com/blog/rss.xml)\n- **–ü–µ—Ä–µ–≤–∞–≥–∏:** –ù–∞–π–Ω–∞–¥—ñ–π–Ω—ñ—à–∏–π, –æ—Ñ—ñ—Ü—ñ–π–Ω–∏–π —Å–ø–æ—Å—ñ–±\n- **–î–∞–Ω—ñ:** –ó–∞–≥–æ–ª–æ–≤–æ–∫, URL, –æ–ø–∏—Å, –¥–∞—Ç–∞ –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó\n- **–†–µ–∑—É–ª—å—Ç–∞—Ç:** DataFrame –∑ 4 –∫–æ–ª–æ–Ω–∫–∞–º–∏\n\n### **2. `scrape_datacamp_working()` ‚Äî HTML –ø–∞—Ä—Å–∏–Ω–≥**\n- **–ú–µ—Ç–æ–¥:** 3 –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó –ø–æ—à—É–∫—É\n  - –ú–µ—Ç–æ–¥ 1: –ü–æ—à—É–∫ `<article>` —Ç–µ–≥—ñ–≤\n  - –ú–µ—Ç–æ–¥ 2: –ü–æ—à—É–∫ `<div>` –∑ –∫–ª–∞—Å–∞–º–∏ (post, card, item)\n  - –ú–µ—Ç–æ–¥ 3: –ó–∞–≥–∞–ª—å–Ω–∏–π –ø–æ—à—É–∫ –ø–æ—Å–∏–ª–∞–Ω—å `/blog/`\n- **–ü–µ—Ä–µ–≤–∞–≥–∏:** Fallback —è–∫—â–æ RSS –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π\n- **–î–∞–Ω—ñ:** –ó–∞–≥–æ–ª–æ–≤–æ–∫, URL\n- **–†–µ–∑—É–ª—å—Ç–∞—Ç:** DataFrame –∑ 2 –∫–æ–ª–æ–Ω–∫–∞–º–∏\n\n### **3. `main()` ‚Äî –ì–æ–ª–æ–≤–Ω–∏–π orchestrator**\n- –ü—Ä–æ–±—É—î —Å–ø–æ—á–∞—Ç–∫—É RSS (–Ω–∞–π–∫—Ä–∞—â–∏–π –º–µ—Ç–æ–¥)\n- –Ø–∫—â–æ RSS fails ‚Üí –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç—å –Ω–∞ HTML –ø–∞—Ä—Å–∏–Ω–≥\n- –ó–±–µ—Ä—ñ–≥–∞—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –≤ CSV —Ñ–∞–π–ª–∏\n- –í–∏–≤–æ–¥–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–∞ –ø–µ—Ä—à—ñ 5 —Å—Ç–∞—Ç–µ–π\n\n## üì¶ –¢–µ—Ö–Ω–æ–ª–æ–≥—ñ—ó\n- **requests** ‚Äî HTTP –∑–∞–ø–∏—Ç–∏\n- **BeautifulSoup4** ‚Äî HTML/XML –ø–∞—Ä—Å–∏–Ω–≥\n- **pandas** ‚Äî –æ–±—Ä–æ–±–∫–∞ –¥–∞–Ω–∏—Ö\n- **lxml** ‚Äî XML parser –¥–ª—è RSS\n","metadata":{},"id":"9ebf8115-aa8f-4e92-808d-1c8f88cea51b","cell_type":"markdown"},{"source":"import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport time\nimport json\n\ndef scrape_datacamp_working():\n    \"\"\"\n    –†–æ–±–æ—á–∞ –≤–µ—Ä—Å—ñ—è –ø–∞—Ä—Å–µ—Ä–∞ DataCamp Blog\n    \"\"\"\n    url = \"https://www.datacamp.com/blog\"\n    \n    # –ë—ñ–ª—å—à —Ä–µ–∞–ª—ñ—Å—Ç–∏—á–Ω—ñ headers\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n        'Accept-Language': 'en-US,en;q=0.5',\n        'Accept-Encoding': 'gzip, deflate, br',\n        'Connection': 'keep-alive',\n        'Upgrade-Insecure-Requests': '1',\n        'Sec-Fetch-Dest': 'document',\n        'Sec-Fetch-Mode': 'navigate',\n        'Sec-Fetch-Site': 'none',\n        'Cache-Control': 'max-age=0'\n    }\n    \n    try:\n        print(\"üîç –í—ñ–¥–ø—Ä–∞–≤–ª—è—î–º–æ –∑–∞–ø–∏—Ç –¥–æ DataCamp Blog...\")\n        response = requests.get(url, headers=headers, timeout=10)\n        response.raise_for_status()\n        \n        print(f\"‚úÖ –û—Ç—Ä–∏–º–∞–Ω–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—å: {response.status_code}\")\n        print(f\"üìÑ –†–æ–∑–º—ñ—Ä —Å—Ç–æ—Ä—ñ–Ω–∫–∏: {len(response.text)} —Å–∏–º–≤–æ–ª—ñ–≤\")\n        \n        soup = BeautifulSoup(response.text, 'html.parser')\n        \n        articles = []\n        \n        # –ú–µ—Ç–æ–¥ 1: –®—É–∫–∞—î–º–æ article tags\n        article_tags = soup.find_all('article')\n        print(f\"üîé –ó–Ω–∞–π–¥–µ–Ω–æ <article> —Ç–µ–≥—ñ–≤: {len(article_tags)}\")\n        \n        for article in article_tags:\n            # –®—É–∫–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫\n            title_tag = article.find(['h2', 'h3', 'h4'], class_=lambda x: x and ('title' in x.lower() or 'heading' in x.lower()))\n            \n            if not title_tag:\n                title_tag = article.find(['h2', 'h3', 'h4'])\n            \n            # –®—É–∫–∞—î–º–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è\n            link_tag = article.find('a', href=True)\n            \n            if title_tag and link_tag:\n                title = title_tag.get_text(strip=True)\n                url = link_tag.get('href')\n                \n                # –§–æ—Ä–º—É—î–º–æ –ø–æ–≤–Ω–∏–π URL —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ\n                if url.startswith('/'):\n                    url = f\"https://www.datacamp.com{url}\"\n                \n                if len(title) > 10 and '/blog/' in url:\n                    articles.append({\n                        'title': title,\n                        'url': url\n                    })\n        \n        # –ú–µ—Ç–æ–¥ 2: –Ø–∫—â–æ article tags –Ω–µ —Å–ø—Ä–∞—Ü—é–≤–∞–ª–∏, —à—É–∫–∞—î–º–æ div –∑ –∫–ª–∞—Å–∞–º–∏\n        if len(articles) == 0:\n            print(\"‚ö†Ô∏è Article tags –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ, –ø—Ä–æ–±—É—î–º–æ —ñ–Ω—à–∏–π –ø—ñ–¥—Ö—ñ–¥...\")\n            \n            # –®—É–∫–∞—î–º–æ –≤—Å—ñ div –∑ –º–æ–∂–ª–∏–≤–∏–º–∏ –∫–ª–∞—Å–∞–º–∏ –±–ª–æ–≥—É\n            blog_items = soup.find_all('div', class_=lambda x: x and any(\n                keyword in x.lower() for keyword in ['post', 'card', 'item', 'article']\n            ))\n            \n            print(f\"üîé –ó–Ω–∞–π–¥–µ–Ω–æ blog items: {len(blog_items)}\")\n            \n            for item in blog_items:\n                link = item.find('a', href=True)\n                if link and '/blog/' in link.get('href'):\n                    title_elem = item.find(['h2', 'h3', 'h4', 'h5'])\n                    \n                    if title_elem:\n                        title = title_elem.get_text(strip=True)\n                        url = link.get('href')\n                        \n                        if url.startswith('/'):\n                            url = f\"https://www.datacamp.com{url}\"\n                        \n                        if len(title) > 10:\n                            articles.append({\n                                'title': title,\n                                'url': url\n                            })\n        \n        # –ú–µ—Ç–æ–¥ 3: –®—É–∫–∞—î–º–æ –≤—Å—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è —è–∫—ñ –≤–µ–¥—É—Ç—å –Ω–∞ /blog/\n        if len(articles) == 0:\n            print(\"‚ö†Ô∏è –ü—Ä–æ–±—É—î–º–æ –∑–∞–≥–∞–ª—å–Ω–∏–π –ø–æ—à—É–∫ –ø–æ—Å–∏–ª–∞–Ω—å...\")\n            \n            all_links = soup.find_all('a', href=lambda x: x and '/blog/' in x and x.count('/') >= 3)\n            \n            print(f\"üîé –ó–Ω–∞–π–¥–µ–Ω–æ –ø–æ—Å–∏–ª–∞–Ω—å –Ω–∞ –±–ª–æ–≥: {len(all_links)}\")\n            \n            seen_urls = set()\n            \n            for link in all_links:\n                url = link.get('href')\n                \n                if url.startswith('/'):\n                    url = f\"https://www.datacamp.com{url}\"\n                \n                # –£–Ω–∏–∫–∞—î–º–æ –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤\n                if url in seen_urls:\n                    continue\n                \n                seen_urls.add(url)\n                \n                # –®—É–∫–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫\n                title = link.get_text(strip=True)\n                \n                # –Ø–∫—â–æ —Ç–µ–∫—Å—Ç –ø–æ—Å–∏–ª–∞–Ω–Ω—è –∑–∞–Ω–∞–¥—Ç–æ –∫–æ—Ä–æ—Ç–∫–∏–π, —à—É–∫–∞—î–º–æ –±–∞—Ç—å–∫—ñ–≤—Å—å–∫–∏–π –µ–ª–µ–º–µ–Ω—Ç\n                if len(title) < 10:\n                    parent = link.find_parent(['div', 'article', 'section'])\n                    if parent:\n                        title_tag = parent.find(['h2', 'h3', 'h4', 'h5'])\n                        if title_tag:\n                            title = title_tag.get_text(strip=True)\n                \n                if len(title) > 10 and len(title) < 200:\n                    articles.append({\n                        'title': title,\n                        'url': url\n                    })\n        \n        # –í–∏–¥–∞–ª—è—î–º–æ –¥—É–±–ª—ñ–∫–∞—Ç–∏\n        unique_articles = []\n        seen_urls = set()\n        \n        for article in articles:\n            if article['url'] not in seen_urls:\n                seen_urls.add(article['url'])\n                unique_articles.append(article)\n        \n        print(f\"\\n‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö —Å—Ç–∞—Ç–µ–π: {len(unique_articles)}\")\n        \n        return pd.DataFrame(unique_articles)\n    \n    except requests.exceptions.RequestException as e:\n        print(f\"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Ç—ñ: {e}\")\n        return pd.DataFrame()\n    except Exception as e:\n        print(f\"‚ùå –ù–µ—Å–ø–æ–¥—ñ–≤–∞–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}\")\n        return pd.DataFrame()\n\n\ndef scrape_datacamp_alternative():\n    \"\"\"\n    –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∏–π –º–µ—Ç–æ–¥: –ø–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ API –∞–±–æ RSS\n    \"\"\"\n    # DataCamp –º–∞—î RSS feed\n    rss_url = \"https://www.datacamp.com/blog/rss.xml\"\n    \n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n    }\n    \n    try:\n        print(\"üîç –ü—Ä–æ–±—É—î–º–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ RSS feed...\")\n        response = requests.get(rss_url, headers=headers, timeout=10)\n        response.raise_for_status()\n        \n        soup = BeautifulSoup(response.text, 'xml')\n        \n        articles = []\n        items = soup.find_all('item')\n        \n        print(f\"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π –≤ RSS: {len(items)}\")\n        \n        for item in items:\n            title = item.find('title').get_text(strip=True) if item.find('title') else 'No title'\n            link = item.find('link').get_text(strip=True) if item.find('link') else ''\n            description = item.find('description').get_text(strip=True) if item.find('description') else ''\n            pub_date = item.find('pubDate').get_text(strip=True) if item.find('pubDate') else ''\n            \n            articles.append({\n                'title': title,\n                'url': link,\n                'description': description,\n                'published': pub_date\n            })\n        \n        return pd.DataFrame(articles)\n    \n    except Exception as e:\n        print(f\"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ RSS: {e}\")\n        return pd.DataFrame()\n\n\ndef main():\n    \"\"\"\n    –ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è\n    \"\"\"\n    print(\"=\" * 60)\n    print(\"üì∞ DataCamp Blog Scraper\")\n    print(\"=\" * 60)\n    \n    # –°–ø–æ—á–∞—Ç–∫—É –ø—Ä–æ–±—É—î–º–æ RSS (–Ω–∞–π–±—ñ–ª—å—à –Ω–∞–¥—ñ–π–Ω–∏–π –º–µ—Ç–æ–¥)\n    print(\"\\nüîÑ –ú–µ—Ç–æ–¥ 1: RSS Feed\")\n    print(\"-\" * 60)\n    df_rss = scrape_datacamp_alternative()\n    \n    if len(df_rss) > 0:\n        print(f\"\\n‚úÖ RSS —É—Å–ø—ñ—à–Ω–æ! –ó—ñ–±—Ä–∞–Ω–æ {len(df_rss)} —Å—Ç–∞—Ç–µ–π\")\n        df_rss.to_csv('datacamp_articles_rss.csv', index=False, encoding='utf-8')\n        print(\"üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–æ –≤: datacamp_articles_rss.csv\")\n        print(\"\\nüìã –ü–µ—Ä—à—ñ 5 —Å—Ç–∞—Ç–µ–π:\")\n        print(df_rss.head())\n        return\n    \n    # –Ø–∫—â–æ RSS –Ω–µ —Å–ø—Ä–∞—Ü—é–≤–∞–≤, –ø—Ä–æ–±—É—î–º–æ HTML –ø–∞—Ä—Å–∏–Ω–≥\n    print(\"\\nüîÑ –ú–µ—Ç–æ–¥ 2: HTML Parsing\")\n    print(\"-\" * 60)\n    df_html = scrape_datacamp_working()\n    \n    if len(df_html) > 0:\n        print(f\"\\n‚úÖ HTML –ø–∞—Ä—Å–∏–Ω–≥ —É—Å–ø—ñ—à–Ω–∏–π! –ó—ñ–±—Ä–∞–Ω–æ {len(df_html)} —Å—Ç–∞—Ç–µ–π\")\n        df_html.to_csv('datacamp_articles_html.csv', index=False, encoding='utf-8')\n        print(\"üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–æ –≤: datacamp_articles_html.csv\")\n        print(\"\\nüìã –ü–µ—Ä—à—ñ 5 —Å—Ç–∞—Ç–µ–π:\")\n        print(df_html.head())\n    else:\n        print(\"\\n‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑—ñ–±—Ä–∞—Ç–∏ –¥–∞–Ω—ñ –∂–æ–¥–Ω–∏–º –º–µ—Ç–æ–¥–æ–º\")\n        print(\"\\nüí° –ú–æ–∂–ª–∏–≤—ñ –ø—Ä–∏—á–∏–Ω–∏:\")\n        print(\"   1. –°–∞–π—Ç –±–ª–æ–∫—É—î —Å–∫—Ä–∞–ø—ñ–Ω–≥\")\n        print(\"   2. –ó–º—ñ–Ω–∏–ª–∞—Å—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ HTML\")\n        print(\"   3. –ü–æ—Ç—Ä—ñ–±–µ–Ω JavaScript –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–æ–Ω—Ç–µ–Ω—Ç—É\")\n        print(\"\\nüîß –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó:\")\n        print(\"   - –í–∏–∫–æ—Ä–∏—Å—Ç–∞–π—Ç–µ Selenium –¥–ª—è dynamic –∫–æ–Ω—Ç–µ–Ω—Ç—É\")\n        print(\"   - –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ robots.txt: https://www.datacamp.com/robots.txt\")\n        print(\"   - –†–æ–∑–≥–ª—è–Ω—å—Ç–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –æ—Ñ—ñ—Ü—ñ–π–Ω–æ–≥–æ API (—è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω–∏–π)\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"executionCancelledAt":null,"executionTime":383,"lastExecutedAt":1761238244361,"lastExecutedByKernel":"35e27d39-3514-4e1b-940e-81359c42024e","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport time\nimport json\n\ndef scrape_datacamp_working():\n    \"\"\"\n    –†–æ–±–æ—á–∞ –≤–µ—Ä—Å—ñ—è –ø–∞—Ä—Å–µ—Ä–∞ DataCamp Blog\n    \"\"\"\n    url = \"https://www.datacamp.com/blog\"\n    \n    # –ë—ñ–ª—å—à —Ä–µ–∞–ª—ñ—Å—Ç–∏—á–Ω—ñ headers\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n        'Accept-Language': 'en-US,en;q=0.5',\n        'Accept-Encoding': 'gzip, deflate, br',\n        'Connection': 'keep-alive',\n        'Upgrade-Insecure-Requests': '1',\n        'Sec-Fetch-Dest': 'document',\n        'Sec-Fetch-Mode': 'navigate',\n        'Sec-Fetch-Site': 'none',\n        'Cache-Control': 'max-age=0'\n    }\n    \n    try:\n        print(\"üîç –í—ñ–¥–ø—Ä–∞–≤–ª—è—î–º–æ –∑–∞–ø–∏—Ç –¥–æ DataCamp Blog...\")\n        response = requests.get(url, headers=headers, timeout=10)\n        response.raise_for_status()\n        \n        print(f\"‚úÖ –û—Ç—Ä–∏–º–∞–Ω–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—å: {response.status_code}\")\n        print(f\"üìÑ –†–æ–∑–º—ñ—Ä —Å—Ç–æ—Ä—ñ–Ω–∫–∏: {len(response.text)} —Å–∏–º–≤–æ–ª—ñ–≤\")\n        \n        soup = BeautifulSoup(response.text, 'html.parser')\n        \n        articles = []\n        \n        # –ú–µ—Ç–æ–¥ 1: –®—É–∫–∞—î–º–æ article tags\n        article_tags = soup.find_all('article')\n        print(f\"üîé –ó–Ω–∞–π–¥–µ–Ω–æ <article> —Ç–µ–≥—ñ–≤: {len(article_tags)}\")\n        \n        for article in article_tags:\n            # –®—É–∫–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫\n            title_tag = article.find(['h2', 'h3', 'h4'], class_=lambda x: x and ('title' in x.lower() or 'heading' in x.lower()))\n            \n            if not title_tag:\n                title_tag = article.find(['h2', 'h3', 'h4'])\n            \n            # –®—É–∫–∞—î–º–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è\n            link_tag = article.find('a', href=True)\n            \n            if title_tag and link_tag:\n                title = title_tag.get_text(strip=True)\n                url = link_tag.get('href')\n                \n                # –§–æ—Ä–º—É—î–º–æ –ø–æ–≤–Ω–∏–π URL —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ\n                if url.startswith('/'):\n                    url = f\"https://www.datacamp.com{url}\"\n                \n                if len(title) > 10 and '/blog/' in url:\n                    articles.append({\n                        'title': title,\n                        'url': url\n                    })\n        \n        # –ú–µ—Ç–æ–¥ 2: –Ø–∫—â–æ article tags –Ω–µ —Å–ø—Ä–∞—Ü—é–≤–∞–ª–∏, —à—É–∫–∞—î–º–æ div –∑ –∫–ª–∞—Å–∞–º–∏\n        if len(articles) == 0:\n            print(\"‚ö†Ô∏è Article tags –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ, –ø—Ä–æ–±—É—î–º–æ —ñ–Ω—à–∏–π –ø—ñ–¥—Ö—ñ–¥...\")\n            \n            # –®—É–∫–∞—î–º–æ –≤—Å—ñ div –∑ –º–æ–∂–ª–∏–≤–∏–º–∏ –∫–ª–∞—Å–∞–º–∏ –±–ª–æ–≥—É\n            blog_items = soup.find_all('div', class_=lambda x: x and any(\n                keyword in x.lower() for keyword in ['post', 'card', 'item', 'article']\n            ))\n            \n            print(f\"üîé –ó–Ω–∞–π–¥–µ–Ω–æ blog items: {len(blog_items)}\")\n            \n            for item in blog_items:\n                link = item.find('a', href=True)\n                if link and '/blog/' in link.get('href'):\n                    title_elem = item.find(['h2', 'h3', 'h4', 'h5'])\n                    \n                    if title_elem:\n                        title = title_elem.get_text(strip=True)\n                        url = link.get('href')\n                        \n                        if url.startswith('/'):\n                            url = f\"https://www.datacamp.com{url}\"\n                        \n                        if len(title) > 10:\n                            articles.append({\n                                'title': title,\n                                'url': url\n                            })\n        \n        # –ú–µ—Ç–æ–¥ 3: –®—É–∫–∞—î–º–æ –≤—Å—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è —è–∫—ñ –≤–µ–¥—É—Ç—å –Ω–∞ /blog/\n        if len(articles) == 0:\n            print(\"‚ö†Ô∏è –ü—Ä–æ–±—É—î–º–æ –∑–∞–≥–∞–ª—å–Ω–∏–π –ø–æ—à—É–∫ –ø–æ—Å–∏–ª–∞–Ω—å...\")\n            \n            all_links = soup.find_all('a', href=lambda x: x and '/blog/' in x and x.count('/') >= 3)\n            \n            print(f\"üîé –ó–Ω–∞–π–¥–µ–Ω–æ –ø–æ—Å–∏–ª–∞–Ω—å –Ω–∞ –±–ª–æ–≥: {len(all_links)}\")\n            \n            seen_urls = set()\n            \n            for link in all_links:\n                url = link.get('href')\n                \n                if url.startswith('/'):\n                    url = f\"https://www.datacamp.com{url}\"\n                \n                # –£–Ω–∏–∫–∞—î–º–æ –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤\n                if url in seen_urls:\n                    continue\n                \n                seen_urls.add(url)\n                \n                # –®—É–∫–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫\n                title = link.get_text(strip=True)\n                \n                # –Ø–∫—â–æ —Ç–µ–∫—Å—Ç –ø–æ—Å–∏–ª–∞–Ω–Ω—è –∑–∞–Ω–∞–¥—Ç–æ –∫–æ—Ä–æ—Ç–∫–∏–π, —à—É–∫–∞—î–º–æ –±–∞—Ç—å–∫—ñ–≤—Å—å–∫–∏–π –µ–ª–µ–º–µ–Ω—Ç\n                if len(title) < 10:\n                    parent = link.find_parent(['div', 'article', 'section'])\n                    if parent:\n                        title_tag = parent.find(['h2', 'h3', 'h4', 'h5'])\n                        if title_tag:\n                            title = title_tag.get_text(strip=True)\n                \n                if len(title) > 10 and len(title) < 200:\n                    articles.append({\n                        'title': title,\n                        'url': url\n                    })\n        \n        # –í–∏–¥–∞–ª—è—î–º–æ –¥—É–±–ª—ñ–∫–∞—Ç–∏\n        unique_articles = []\n        seen_urls = set()\n        \n        for article in articles:\n            if article['url'] not in seen_urls:\n                seen_urls.add(article['url'])\n                unique_articles.append(article)\n        \n        print(f\"\\n‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö —Å—Ç–∞—Ç–µ–π: {len(unique_articles)}\")\n        \n        return pd.DataFrame(unique_articles)\n    \n    except requests.exceptions.RequestException as e:\n        print(f\"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Ç—ñ: {e}\")\n        return pd.DataFrame()\n    except Exception as e:\n        print(f\"‚ùå –ù–µ—Å–ø–æ–¥—ñ–≤–∞–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}\")\n        return pd.DataFrame()\n\n\ndef scrape_datacamp_alternative():\n    \"\"\"\n    –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∏–π –º–µ—Ç–æ–¥: –ø–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ API –∞–±–æ RSS\n    \"\"\"\n    # DataCamp –º–∞—î RSS feed\n    rss_url = \"https://www.datacamp.com/blog/rss.xml\"\n    \n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n    }\n    \n    try:\n        print(\"üîç –ü—Ä–æ–±—É—î–º–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ RSS feed...\")\n        response = requests.get(rss_url, headers=headers, timeout=10)\n        response.raise_for_status()\n        \n        soup = BeautifulSoup(response.text, 'xml')\n        \n        articles = []\n        items = soup.find_all('item')\n        \n        print(f\"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π –≤ RSS: {len(items)}\")\n        \n        for item in items:\n            title = item.find('title').get_text(strip=True) if item.find('title') else 'No title'\n            link = item.find('link').get_text(strip=True) if item.find('link') else ''\n            description = item.find('description').get_text(strip=True) if item.find('description') else ''\n            pub_date = item.find('pubDate').get_text(strip=True) if item.find('pubDate') else ''\n            \n            articles.append({\n                'title': title,\n                'url': link,\n                'description': description,\n                'published': pub_date\n            })\n        \n        return pd.DataFrame(articles)\n    \n    except Exception as e:\n        print(f\"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ RSS: {e}\")\n        return pd.DataFrame()\n\n\ndef main():\n    \"\"\"\n    –ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è\n    \"\"\"\n    print(\"=\" * 60)\n    print(\"üì∞ DataCamp Blog Scraper\")\n    print(\"=\" * 60)\n    \n    # –°–ø–æ—á–∞—Ç–∫—É –ø—Ä–æ–±—É—î–º–æ RSS (–Ω–∞–π–±—ñ–ª—å—à –Ω–∞–¥—ñ–π–Ω–∏–π –º–µ—Ç–æ–¥)\n    print(\"\\nüîÑ –ú–µ—Ç–æ–¥ 1: RSS Feed\")\n    print(\"-\" * 60)\n    df_rss = scrape_datacamp_alternative()\n    \n    if len(df_rss) > 0:\n        print(f\"\\n‚úÖ RSS —É—Å–ø—ñ—à–Ω–æ! –ó—ñ–±—Ä–∞–Ω–æ {len(df_rss)} —Å—Ç–∞—Ç–µ–π\")\n        df_rss.to_csv('datacamp_articles_rss.csv', index=False, encoding='utf-8')\n        print(\"üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–æ –≤: datacamp_articles_rss.csv\")\n        print(\"\\nüìã –ü–µ—Ä—à—ñ 5 —Å—Ç–∞—Ç–µ–π:\")\n        print(df_rss.head())\n        return\n    \n    # –Ø–∫—â–æ RSS –Ω–µ —Å–ø—Ä–∞—Ü—é–≤–∞–≤, –ø—Ä–æ–±—É—î–º–æ HTML –ø–∞—Ä—Å–∏–Ω–≥\n    print(\"\\nüîÑ –ú–µ—Ç–æ–¥ 2: HTML Parsing\")\n    print(\"-\" * 60)\n    df_html = scrape_datacamp_working()\n    \n    if len(df_html) > 0:\n        print(f\"\\n‚úÖ HTML –ø–∞—Ä—Å–∏–Ω–≥ —É—Å–ø—ñ—à–Ω–∏–π! –ó—ñ–±—Ä–∞–Ω–æ {len(df_html)} —Å—Ç–∞—Ç–µ–π\")\n        df_html.to_csv('datacamp_articles_html.csv', index=False, encoding='utf-8')\n        print(\"üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–æ –≤: datacamp_articles_html.csv\")\n        print(\"\\nüìã –ü–µ—Ä—à—ñ 5 —Å—Ç–∞—Ç–µ–π:\")\n        print(df_html.head())\n    else:\n        print(\"\\n‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑—ñ–±—Ä–∞—Ç–∏ –¥–∞–Ω—ñ –∂–æ–¥–Ω–∏–º –º–µ—Ç–æ–¥–æ–º\")\n        print(\"\\nüí° –ú–æ–∂–ª–∏–≤—ñ –ø—Ä–∏—á–∏–Ω–∏:\")\n        print(\"   1. –°–∞–π—Ç –±–ª–æ–∫—É—î —Å–∫—Ä–∞–ø—ñ–Ω–≥\")\n        print(\"   2. –ó–º—ñ–Ω–∏–ª–∞—Å—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ HTML\")\n        print(\"   3. –ü–æ—Ç—Ä—ñ–±–µ–Ω JavaScript –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–æ–Ω—Ç–µ–Ω—Ç—É\")\n        print(\"\\nüîß –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó:\")\n        print(\"   - –í–∏–∫–æ—Ä–∏—Å—Ç–∞–π—Ç–µ Selenium –¥–ª—è dynamic –∫–æ–Ω—Ç–µ–Ω—Ç—É\")\n        print(\"   - –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ robots.txt: https://www.datacamp.com/robots.txt\")\n        print(\"   - –†–æ–∑–≥–ª—è–Ω—å—Ç–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –æ—Ñ—ñ—Ü—ñ–π–Ω–æ–≥–æ API (—è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω–∏–π)\")\n\n\nif __name__ == \"__main__\":\n    main()","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"id":"de35a2a5-6338-4e72-9687-5194a6309eba","cell_type":"code","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":"============================================================\nüì∞ DataCamp Blog Scraper\n============================================================\n\nüîÑ –ú–µ—Ç–æ–¥ 1: RSS Feed\n------------------------------------------------------------\nüîç –ü—Ä–æ–±—É—î–º–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ RSS feed...\n‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ RSS: 404 Client Error: Not Found for url: https://www.datacamp.com/blog/rss.xml\n\nüîÑ –ú–µ—Ç–æ–¥ 2: HTML Parsing\n------------------------------------------------------------\nüîç –í—ñ–¥–ø—Ä–∞–≤–ª—è—î–º–æ –∑–∞–ø–∏—Ç –¥–æ DataCamp Blog...\n‚úÖ –û—Ç—Ä–∏–º–∞–Ω–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—å: 200\nüìÑ –†–æ–∑–º—ñ—Ä —Å—Ç–æ—Ä—ñ–Ω–∫–∏: 414631 —Å–∏–º–≤–æ–ª—ñ–≤\nüîé –ó–Ω–∞–π–¥–µ–Ω–æ <article> —Ç–µ–≥—ñ–≤: 0\n‚ö†Ô∏è Article tags –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ, –ø—Ä–æ–±—É—î–º–æ —ñ–Ω—à–∏–π –ø—ñ–¥—Ö—ñ–¥...\nüîé –ó–Ω–∞–π–¥–µ–Ω–æ blog items: 0\n‚ö†Ô∏è –ü—Ä–æ–±—É—î–º–æ –∑–∞–≥–∞–ª—å–Ω–∏–π –ø–æ—à—É–∫ –ø–æ—Å–∏–ª–∞–Ω—å...\nüîé –ó–Ω–∞–π–¥–µ–Ω–æ –ø–æ—Å–∏–ª–∞–Ω—å –Ω–∞ –±–ª–æ–≥: 83\n\n‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö —Å—Ç–∞—Ç–µ–π: 25\n\n‚úÖ HTML –ø–∞—Ä—Å–∏–Ω–≥ —É—Å–ø—ñ—à–Ω–∏–π! –ó—ñ–±—Ä–∞–Ω–æ 25 —Å—Ç–∞—Ç–µ–π\nüíæ –ó–±–µ—Ä–µ–∂–µ–Ω–æ –≤: datacamp_articles_html.csv\n\nüìã –ü–µ—Ä—à—ñ 5 —Å—Ç–∞—Ç–µ–π:\n                 title                                                url\n0        Certification  https://www.datacamp.com/blog/category/certifi...\n1  DataCamp Classrooms  https://www.datacamp.com/blog/category/datacam...\n2     DataCamp Donates  https://www.datacamp.com/blog/category/datacam...\n3         For Business  https://www.datacamp.com/blog/category/for-bus...\n4      Learner Stories  https://www.datacamp.com/blog/category/learner...\n"}]},{"source":"import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport time\n\ndef scrape_datacamp_simple():\n    \"\"\"–ü—Ä–æ—Å—Ç–∞ –≤–µ—Ä—Å—ñ—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—ó\"\"\"\n    url = \"https://www.datacamp.com/blog\"\n    \n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n    }\n    \n    response = requests.get(url, headers=headers)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    \n    articles = []\n    \n    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –≤—Å—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ —Å—Ç–∞—Ç—Ç—ñ\n    links = soup.find_all('a', href=True)\n    \n    for link in links:\n        href = link.get('href')\n        if '/blog/' in href and href.count('/') > 3:\n            title = link.get_text(strip=True)\n            if len(title) > 10:\n                articles.append({\n                    'title': title,\n                    'url': href\n                })\n    \n    return pd.DataFrame(articles)\n\n# –ó–±—ñ—Ä –¥–∞–Ω–∏—Ö\ndf = scrape_datacamp_simple()\ndf.to_csv('datacamp_articles.csv', index=False)\nprint(f\"–ó—ñ–±—Ä–∞–Ω–æ {len(df)} —Å—Ç–∞—Ç–µ–π\")","metadata":{"executionCancelledAt":null,"executionTime":193,"lastExecutedAt":1761238405131,"lastExecutedByKernel":"35e27d39-3514-4e1b-940e-81359c42024e","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport time\n\ndef scrape_datacamp_simple():\n    \"\"\"–ü—Ä–æ—Å—Ç–∞ –≤–µ—Ä—Å—ñ—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—ó\"\"\"\n    url = \"https://www.datacamp.com/blog\"\n    \n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n    }\n    \n    response = requests.get(url, headers=headers)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    \n    articles = []\n    \n    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –≤—Å—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ —Å—Ç–∞—Ç—Ç—ñ\n    links = soup.find_all('a', href=True)\n    \n    for link in links:\n        href = link.get('href')\n        if '/blog/' in href and href.count('/') > 3:\n            title = link.get_text(strip=True)\n            if len(title) > 10:\n                articles.append({\n                    'title': title,\n                    'url': href\n                })\n    \n    return pd.DataFrame(articles)\n\n# –ó–±—ñ—Ä –¥–∞–Ω–∏—Ö\ndf = scrape_datacamp_simple()\ndf.to_csv('datacamp_articles.csv', index=False)\nprint(f\"–ó—ñ–±—Ä–∞–Ω–æ {len(df)} —Å—Ç–∞—Ç–µ–π\")","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"id":"4e463e83-8bd1-4969-9678-729b35e49d78","cell_type":"code","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":"–ó—ñ–±—Ä–∞–Ω–æ 0 —Å—Ç–∞—Ç–µ–π\n"}]},{"source":"","metadata":{},"id":"320de3e9-f01c-4df2-9ccb-e7c8eb2e31fa","cell_type":"code","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (User venv)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataLab"},"nbformat":4,"nbformat_minor":5}
